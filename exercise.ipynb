{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import rdkit.Chem\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting boiling points based on generic molecule information.\n",
    "* To test and train a model a data set is mandatory. We will stick to the set accesible on the  \n",
    "    quantitative structureâˆ’activity relationships database (qsarDB): https://qsardb.org/repository/handle/10967/158  \n",
    "\n",
    "* It provides us with boiling points and some other information on the molecule like the number of specific atoms or functional groups and connectivity index.\n",
    "\n",
    "* You can find the boiling points in the file '1999JCIM491/properties/Tb/values' (seperated by tabs)\n",
    "\n",
    "* The for each descriptor is a subfolder in '1999JCIM491/descriptors' where you will also find a 'values' file\n",
    "    * glob.glob('to/dir/*/') provides a list of all subfolders in folder 'to/dir/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import the target and descriptors (features)\n",
    "descriptor_folder = '1999JCIM491/descriptors/'\n",
    "property_file = '1999JCIM491/properties/Tb/values'\n",
    "\n",
    "properties = pd.DataFrame()#import boiling temperature\n",
    "descriptor = pd.DataFrame()#import descriptors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature processing\n",
    "\n",
    "* Typically the features need to be standardized and normalized to achieve a functioning basis for an ML model.\n",
    "\n",
    "* Luckily decision trees do not depend on these, and therefore we do not have to take care of this. \n",
    "\n",
    "* Just keep in mind, that if you use other algorithms the feature processing might be essential."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Training\n",
    "\n",
    "* For validation the data set needs to be divided into a test and training set. \n",
    "\n",
    "* Typically, 75 \\% of the data set is used for training, while the other 25 % are used for testing the model.\n",
    "\n",
    "* The model can than be fitted with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "properties_train,properties_test,descriptors_train,descriptors_test = train_test_split(properties,descriptors,test_size=0.25,random_state=42)\n",
    "\n",
    "Feature_train = descriptors_train\n",
    "\n",
    "Target_train = properties_train\n",
    "\n",
    "MLmodel = DecisionTreeRegressor(max_depth=2)\n",
    "\n",
    "MLmodel.fit(Feature_train,Target_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Analysis\n",
    "\n",
    "* There are several tools and techniques to analyze the quality of an ML model. In the following are some examples\n",
    "\n",
    "* Specific for decision trees is the visualization of them by plotting the tree with an in-build function.  \n",
    "However, this only makes sense for low tree depth.\n",
    "\n",
    "* This makes them easy to interpret and easy to follow the ongoing routines inside the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(MLmodel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction \n",
    "* The test set can be now easily predicted as shown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_test = descriptors_test\n",
    "Target_test = properties_test\n",
    "\n",
    "Pred_Target = MLmodel.predict(Feature_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plots\n",
    "* Furthermore, the tested data can be predicted and compared in a box plot, where the true property is plotted against the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the prediction and the true values of the boiling points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see from this example, the depth of the model is chosen too low and therefore only 4 different values are predicted.\n",
    "\n",
    "* Redo the fit without specifying the maximum depth. In such case, the depth of the tree increases until all leaves are pure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning plots\n",
    "\n",
    "* Examining the learning procedure of a model is basic analysis to see how significant the model benefits from an increase of data.\n",
    "\n",
    "* In a learning plot a measurement of the accuracy of a model is plotted against the amount of data used for the fit.\n",
    "\n",
    "    * The measurement of accuracy can be for example the RMSE or MAE \n",
    "\n",
    "    * To achieve a statistical more significant result do the fit for every training set size for several random seeds\n",
    "\n",
    "    * If the model actually learns from the provided features the accuracy should increase with increasing training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "\n",
    "* ML algorithms rely on several predefined parameters, which all can be optimized.\n",
    "\n",
    "* There are automatized in-build functions in sklearn learn, which allow a grid based search for the best performing parameter ensemble.\n",
    "\n",
    "* Either the complete parameter grid can be screened or a predefined number of random parameter ensembles is tested. \n",
    "\n",
    "* For this the algorithm is wrapped in a search function and a grid or distribution of the parameters has to be provided.\n",
    "\n",
    "### Some info to the given parameters\n",
    "\n",
    "* 'max_depth' gives the maximum depth of the tree\n",
    "\n",
    "* 'ccp_alpha' is a parameter that describes how much the tree is pruned. Pruning is used, as the tree can tend to overfit. Pruning reduces the number of leaves in the tree.\n",
    "\n",
    "* 'max_features' is the number of features when looking at the best split.\n",
    "\n",
    "### Parallelization \n",
    "\n",
    "* As these optimization procedures can get quite costly, it is helpful to use parallelization\n",
    "\n",
    "* This is conveniently implemented in sklearn and all we have to do is to provide a number of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "params = {'max_depth' : [2,5,10,1000000000],\n",
    "          'ccp_alpha' : [False,0.001,0.002,0.005],\n",
    "          'max_features' : [0.5,0.75,1.0]}\n",
    "\n",
    "MLmodel = GridSearchCV(DecisionTreeRegressor(),param_grid=params,n_jobs=8)\n",
    "\n",
    "MLmodel.fit(Feature_train,Target_train)\n",
    "\n",
    "MLmodel.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another possibility for the optimization of the model is to use a more proficient algorithm.\n",
    "\n",
    "*  For example ensemble based methods can be used. These rely on the several decision trees from which an average is constructed for the prediction.\n",
    "\n",
    "* Also for this method a parameter optimization can be useful. Another important quantity is the number of estimators (so of the single decision trees) should be included for the parameter optimization.\n",
    "\n",
    "* Redo the parameter search but with the ensemble method and fit the test system again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "params = {'max_depth' : [2,5,10,100,10000],\n",
    "          'ccp_alpha' : [False,0.001,0.002,0.005],\n",
    "          'max_features' : [0.1,0.25,0.5,0.75,1.0],\n",
    "          'n_estimators' : [50,100,300,500]}\n",
    "\n",
    "MLmodel = GridSearchCV(ExtraTreesRegressor(),param_grid=params)\n",
    "\n",
    "MLmodel.set_params(n_jobs=8)\n",
    "MLmodel.fit(Feature_train,Target_train)\n",
    "MLmodel.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction\n",
    "\n",
    "* Tree based algorithms can also be enhanced by a feature reduction, if the chosen features only have a low variance throughout the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "selected_Feature = selector.fit(Features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features\n",
    "\n",
    "* However, there might be also other features which have a significant influence on the performance.\n",
    "\n",
    "* One could for example also include the masses of the molecule.\n",
    "\n",
    "* The package rdkit provides also functions which return number of specific fragments. \n",
    "\n",
    "* You can access the structure of the molecule via the InChI of each system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import FunctionalGroups,Descriptors,Fragments\n",
    "\n",
    "inchi_file = '1999JCIM491/compounds/compounds.xml'\n",
    "\n",
    "inchis = pd.read_xml(inchi_file,parser='lxml')\n",
    "\n",
    "properties = pd.read_csv(property_file,index_col=0,sep='\\t')\n",
    "\n",
    "wt = []\n",
    "HA_wt = []\n",
    "mols = []\n",
    "hierachy = []\n",
    "\n",
    "for i in range(len(inchis['InChI'])):\n",
    "    a = rdkit.Chem.inchi.MolFromInchi(inchis['InChI'][i],removeHs=True)\n",
    "    mols.append(a)\n",
    "    hierachy.append(i)\n",
    "    wt.append(Descriptors.ExactMolWt(a))\n",
    "    HA_wt.append(Descriptors.HeavyAtomMolWt(a))\n",
    "\n",
    "\n",
    "desc = Fragments.fr_Al_OH(a)\n",
    "\n",
    "descriptors['wt'] = wt\n",
    "descriptors['HA_wt'] = HA_wt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTRenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83ab36e73b4f0e20cc38f06976fa36305ed99d7173385e03369950e8b8f6876a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
